{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from nltk import sent_tokenize\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "class Annotation():\n",
    "    def __init__(self, annotation_xml_object):\n",
    "        self.id = annotation_xml_object.attrib['id']\n",
    "        infons = {a.attrib['key']: a.text for a in annotation_xml_object.findall('infon')}\n",
    "        self.file, self.type = infons['file'], infons['type']\n",
    "        self.location = {i[0]: int(i[1]) for i in annotation_xml_object.find('location').attrib.items()}\n",
    "        self.text = annotation_xml_object.find('text').text\n",
    "        self.encoded_text = None\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'ID: {}\\nLocation: {}\\nText: {}'.format(self.id, self.location, self.text)\n",
    "    \n",
    "    def as_dict(self):\n",
    "        return {'id': self.id, 'type': self.type, 'location': self.location, 'text': self.text}\n",
    "\n",
    "class Relation():    \n",
    "    def __init__(self, rel_xml_object):\n",
    "        self.id = rel_xml_object.attrib['id']\n",
    "        infons = {a.attrib['key']: a.text for a in rel_xml_object.findall('infon')}\n",
    "        self.file, self.type = infons['file'], infons['type']\n",
    "        try:\n",
    "            self.relation_type = infons['relation type']\n",
    "        except KeyError:\n",
    "            self.relation_type = None\n",
    "        self.nodes = {n.attrib['role'].lower(): n.attrib['refid'] for n in rel_xml_object.findall('node')}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Relation {}\\nType {}\\nFrom {} to {}'.format(self.id, self.relation_type, *self.nodes.values())\n",
    "    \n",
    "    def as_dict(self):\n",
    "        return {'id': self.id, 'type': self.type, 'nodes': self.nodes, 'relation_type': self.relation_type}\n",
    "\n",
    "class Document():\n",
    "    def __init__(self, document_xml_object, ignore_text=True):\n",
    "        self.id = document_xml_object[0].text\n",
    "        \n",
    "        self.text = document_xml_object[1][1].text if not ignore_text else ''\n",
    "        self.sentences = sent_tokenize(self.text)\n",
    "        if self.id == '14731280':  # hack to fix a bad sentence split\n",
    "            self.sentences = self.sentences[:3] + [self.sentences[3] + self.sentences[4]] + self.sentences[5:]\n",
    "        \n",
    "        self.annotations = {a.id: a for a in [Annotation(a) for a in document_xml_object[1].findall('annotation')]}\n",
    "        self.__generate_sentence_based_locations_for_annotations()\n",
    "        \n",
    "        self.relations = {r.id: r for r in [Relation(r) for r in document_xml_object[1].findall('relation')]}\n",
    "        self.__create_entity_tokens()\n",
    "        self.__extract_annotation_ids_for_genes()\n",
    "        self.__extract_gene_relations()\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'ID: {}\\nAnnotations: {}\\nRelations: {}'.format(\n",
    "            self._id, len(self.annotations), len(self.relations))\n",
    "    \n",
    "    def __extract_annotation_ids_for_genes(self):\n",
    "        self.gene_ids = set([a.id for a in self.annotations.values() if a.type == 'Gene'])\n",
    "    \n",
    "    def __extract_gene_relations(self):\n",
    "        self.gene_relations = {r.id: r for r in self.relations.values() if\n",
    "            all([_id in self.gene_ids for _id in r.nodes.values()])}\n",
    "    \n",
    "    def __generate_sentence_based_locations_for_annotations(self):\n",
    "        for ann in self.annotations.values():\n",
    "            char_offset = ann.location['offset']\n",
    "            for i, sent in enumerate(self.sentences):\n",
    "                if char_offset - len(sent) < 0:\n",
    "                    ann.location = {'sentence_idx': i, 'char_idx': char_offset, 'length': ann.location['length']}\n",
    "                    break\n",
    "                else:\n",
    "                    char_offset -= len(sent) + 1  # subtract one for the space between 2 sentences\n",
    "    \n",
    "    def __create_entity_tokens(self):\n",
    "        for ann in self.annotations.values():\n",
    "            ann.text = ann.text.replace(' ', '_')\n",
    "            sent_id, start, length = ann.location['sentence_idx'], ann.location['char_idx'], ann.location['length']\n",
    "            sent = self.sentences[sent_id]\n",
    "            self.sentences[sent_id] = sent[:start] + sent[start:start+length].replace(' ', '_') + sent[start+length:]\n",
    "    \n",
    "    def __rel2sentence(self, rels):\n",
    "        sentences = []\n",
    "        for rel in rels:\n",
    "            src = self.annotations[rel.nodes['cause']]\n",
    "            target = self.annotations[rel.nodes['theme']]\n",
    "            src_sentence_id, target_sentence_id = [it.location['sentence_idx'] for it in [src, target]]\n",
    "            \n",
    "            try:\n",
    "                assert src_sentence_id == target_sentence_id\n",
    "            except AssertionError:\n",
    "                pass\n",
    "#                 print(self.id)\n",
    "#                 print(src_sentence_id, target_sentence_id)\n",
    "#                 print((src.id, src.text), (target.id, target.text))\n",
    "#                 print(self.sentences[src_sentence_id])\n",
    "#                 print()\n",
    "#                 print(self.sentences[target_sentence_id])\n",
    "#                 print('---')\n",
    "            sentences.append({\n",
    "                'id': '{}-{}'.format(self.id, src_sentence_id),\n",
    "                'sentence': self.sentences[src_sentence_id],\n",
    "                'src': src.as_dict(), 'target': target.as_dict()\n",
    "            })\n",
    "        return sentences\n",
    "    \n",
    "    def get_sentences_with_gene_relation(self):\n",
    "        return self.__rel2sentence(self.gene_relations.values())\n",
    "    \n",
    "    def get_sentences_without_gene_relation(self):\n",
    "        non_gene_rels = [rel for rel_id, rel in self.relations.items() if rel_id not in self.gene_relations]\n",
    "        return self.__rel2sentence(non_gene_rels)\n",
    "    \n",
    "    def _test_annotation_location_types(self):\n",
    "        for ann in self.annotations.values():\n",
    "            full_text_based = self.text[ann.location['offset']:ann.location['offset']+ann.location['length']]\n",
    "            start, length = ann.location['char_idx'], ann.location['length']\n",
    "            sent_based = self.sentences[ann.location['sentence_idx']][start:start+length]\n",
    "            if not full_text_based == sent_based:\n",
    "                print('-----')\n",
    "                print('DocID: ' + self.id)\n",
    "                print('AnnotationID: ' + ann.id)\n",
    "                print('Should:\\n' + ann.text)\n",
    "                print('Full text:\\n' + full_text_based)\n",
    "                print('Sent based:\\n' + sent_based)\n",
    "                print('----')\n",
    "    \n",
    "class CorpusParser():\n",
    "    def __init__(self, path):\n",
    "        self.tree = ET.parse(path)\n",
    "        self.root = self.tree.getroot()\n",
    "        self.documents = [Document(d, ignore_text=False) for d in self.root if d.tag == 'document']\n",
    "    \n",
    "    def get_sentences_with_annotations(self, genes_only=False):\n",
    "        sentences = []\n",
    "        for d in self.documents:\n",
    "            for sent_idx, sent in enumerate(d.sentences):\n",
    "                sent_dict = {'sentence': sent}\n",
    "                sent_dict['annotations'] = [ann.as_dict() for ann in d.annotations.values()\n",
    "                                            if ann.location['sentence_idx'] == sent_idx \n",
    "                                            and (not genes_only or ann.type == 'Gene')]\n",
    "                ann_ids = set([ann['id'] for ann in sent_dict['annotations']])\n",
    "                sent_dict['relations'] = [rel.as_dict() for rel in d.gene_relations.values()\n",
    "                                          if rel.nodes['cause'] in ann_ids]\n",
    "                sentences.append(sent_dict)    \n",
    "        return sentences\n",
    "    \n",
    "    def get_all_sentences_with_gene_relation(self):\n",
    "        sentences = []\n",
    "        for d in self.documents:\n",
    "            sents = d.get_sentences_with_gene_relation()\n",
    "            if sents is not None:\n",
    "                sentences += sents\n",
    "        return sentences\n",
    "\n",
    "    def get_all_sentences_without_gene_relation(self):\n",
    "        sentences = []\n",
    "        for d in self.documents:\n",
    "            sents = d.get_sentences_without_gene_relation()\n",
    "            if sents is not None:\n",
    "                sentences += sents\n",
    "        return sentences\n",
    "\n",
    "    def get_all_sentences(self):\n",
    "        return sum([d.sentences for d in self.documents], [])\n",
    "    \n",
    "    def get_all_relations(self):\n",
    "        return sum([list(d.relations.values()) for d in self.documents], [])\n",
    "    \n",
    "    def get_all_gene_relations(self):\n",
    "        return sum([list(d.gene_relations.values()) for d in self.documents], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = CorpusParser('genereg_bioc.xml')\n",
    "\n",
    "sents = corpus.get_all_sentences_with_gene_relation()\n",
    "distinct_sents = set([s['id'] for s in sents])\n",
    "len(distinct_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sentence': 'Mechanisms of acid resistance in enterohemorrhagic Escherichia coli.', 'annotations': [], 'relations': []}, {'sentence': 'Enterohemorrhagic strains of Escherichia coli must pass through the acidic gastric barrier to cause gastrointestinal disease.', 'annotations': [], 'relations': []}]\n",
      "2767\n",
      "2767\n",
      "Counter({'PositiveRegulationOfGeneExpression': 465, 'RegulationOfGeneExpression': 417, 'NegativeRegulationOfGeneExpression': 282})\n",
      "1154\n",
      "pos_pairs 1164\n",
      "neg_pairs 1613\n"
     ]
    }
   ],
   "source": [
    "sentences = corpus.get_sentences_with_annotations(genes_only=True)\n",
    "print(sentences[:2])\n",
    "pairs = []\n",
    "relas = []\n",
    "for s in sentences:\n",
    "    anns, rels = s['annotations'], s['relations']\n",
    "    for i in range(len(anns)-1):\n",
    "        for j in range(i+1, len(anns)):\n",
    "            pairs.append((anns[i]['id'], anns[j]['id']))\n",
    "    for r in rels:\n",
    "        relas.append(r)\n",
    "\n",
    "print(len(pairs))\n",
    "pairs = set(pairs)\n",
    "print(len(pairs))\n",
    "\n",
    "pos_pairs, neg_pairs = [], []\n",
    "seen = set()\n",
    "from collections import Counter\n",
    "c = Counter()\n",
    "for r in relas:\n",
    "    src = r['nodes']['cause']\n",
    "    target = r['nodes']['theme']\n",
    "    c.update({r['relation_type']: 1})\n",
    "    if (src, target) in pairs:\n",
    "        pos_pairs.append((src, target))\n",
    "    elif (target, src) in pairs:\n",
    "        pos_pairs.append((target, src))\n",
    "        \n",
    "print(c)\n",
    "neg_pairs = pairs - set(pos_pairs)\n",
    "\n",
    "print(len(set(pos_pairs)))\n",
    "print('pos_pairs', len(pos_pairs))\n",
    "print('neg_pairs', len(neg_pairs))\n",
    "\n",
    "    \n",
    "# print('num_of_rels_in_all_sents', sum([len(s['relations']) for s in sentences]))\n",
    "# print('gene rels_w_sents', len(corpus.get_all_sentences_with_gene_relation()))\n",
    "# print('gene_rels', len(corpus.get_all_gene_relations()))\n",
    "# print('all_rels', len(corpus.get_all_relations()))\n",
    "\n",
    "# len(corpus.get_all_sentences())\n",
    "\n",
    "# j = json.dumps([s for s in sentences if len(s['relations'])>1][:10], indent=2)\n",
    "# j = j.split('\\n')\n",
    "# for l in j:\n",
    "#     print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Genereg corpus...\n",
      "...done\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from bllipparser import RerankingParser\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # might be a bad solution\n",
    "\n",
    "print('Load Genereg corpus...')\n",
    "corpus = CorpusParser('genereg_bioc.xml')\n",
    "print('...done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load RerankingParser...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parser is already loaded and can only be loaded once.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-4af98c57a258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Load RerankingParser...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRerankingParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_unified_model_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../ppi-benchmark/Parsing/Models/McClosky-2009/biomodel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'...done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Load lemmatizer...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/bllipparser/RerankingParser.py\u001b[0m in \u001b[0;36mfrom_unified_model_dir\u001b[0;34m(this_class, model_dir, parsing_options, reranker_options, parser_only)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mrrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparser_model_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0mrrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_parser_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser_model_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparsing_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreranker_features_filename\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreranker_weights_filename\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m            \u001b[0;32mnot\u001b[0m \u001b[0mparser_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/bllipparser/RerankingParser.py\u001b[0m in \u001b[0;36mload_parser_model\u001b[0;34m(self, model_dir, terms_only, heads_only, **parser_options)\u001b[0m\n\u001b[1;32m    574\u001b[0m         be loaded but the full parsing model will not.\"\"\"\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRerankingParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parser_model_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             raise RuntimeError('Parser is already loaded and can only '\n\u001b[0m\u001b[1;32m    577\u001b[0m                                'be loaded once.')\n\u001b[1;32m    578\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_path_or_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Parser model directory'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parser is already loaded and can only be loaded once."
     ]
    }
   ],
   "source": [
    "print('Load RerankingParser...')\n",
    "rrp = RerankingParser.from_unified_model_dir('../../ppi-benchmark/Parsing/Models/McClosky-2009/biomodel')\n",
    "print('...done')\n",
    "print('Load lemmatizer...')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print('...done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tokens_to_original_sentence(sentence, tokens):\n",
    "    '''Annotate tokens with their starting position in the original sentence'''\n",
    "    cur_char_idx, cur_token_idx = 0, 0\n",
    "    new_tokens = []\n",
    "    while cur_token_idx < len(tokens):\n",
    "        cur_token_text = tokens[cur_token_idx]['text']\n",
    "        if sentence[cur_char_idx:].startswith(cur_token_text):\n",
    "            # consume our token\n",
    "            new_token = {k:v for k,v in tokens[cur_token_idx].items()}\n",
    "            new_token['start'] = cur_char_idx\n",
    "            new_tokens.append(new_token)\n",
    "            cur_char_idx += len(cur_token_text)\n",
    "            cur_token_idx += 1\n",
    "        elif sentence[cur_char_idx:cur_char_idx+1] == ' ':\n",
    "            # we have a whitespace that got lost during tokenization\n",
    "            cur_char_idx += 1\n",
    "        elif cur_token_text in ['-LRB-', '-RRB-']:  # more special chars will be needed\n",
    "            # we just forget about these chars (assumption being that they are never part of an entity)\n",
    "            cur_token_idx += 1\n",
    "            cur_char_idx += 1\n",
    "\n",
    "    return new_tokens      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tag_tokens_with_annotations(tokens_with_pos, annotations):\n",
    "    '''\n",
    "    Given annotations, tag and modify tokens based on their position in the original sentence.\n",
    "    tokens_with_pos should be a dict of the form {'start': start pos in sentence, 'text': token's text}\n",
    "    '''\n",
    "\n",
    "    def get_tokens_touched_by_annotation(ann_start, ann_length, tokens_with_pos):\n",
    "        token_candidates = []\n",
    "        for idx, t in enumerate(tokens_with_pos):\n",
    "            positions_of_token = set(range(t['start'], t['start']+len(t['text'])))\n",
    "            positions_of_entity = set(range(ann_start, ann_start+ann_length))\n",
    "            if len(positions_of_token & positions_of_entity) > 0:\n",
    "                token_candidates.append((idx, t))\n",
    "        return token_candidates\n",
    "\n",
    "    for ann in annotations:\n",
    "        ann_start, ann_length = ann['location']['char_idx'], ann['location']['length']\n",
    "        token_candidates = get_tokens_touched_by_annotation(ann_start, ann_length, tokens_with_pos)\n",
    "        token_idxs = token_candidates[0][0], token_candidates[-1][0]  # indizes of tokens touched\n",
    "\n",
    "        # merge if multiple tokens are touched\n",
    "        merged_token_text = ''.join([t['text'] for _, t in token_candidates])\n",
    "        merged_token = {\n",
    "            'start': token_candidates[0][1]['start'],\n",
    "            'text': merged_token_text,\n",
    "            'pos': token_candidates[0][1]['pos'],  # heuristic: take the first one\n",
    "            'entity': ann['text'],\n",
    "            'entity_id': ann['id']\n",
    "        }\n",
    "\n",
    "        # handle special cases where an entity does not align with the token\n",
    "        prefix_token, suffix_token = None, None\n",
    "\n",
    "        prefix_diff = ann_start - merged_token['start']\n",
    "        if prefix_diff > 0:\n",
    "            # cut off prefix and put in new token\n",
    "            prefix_token = {'start': merged_token['start'],\n",
    "                            'text': merged_token['text'][:prefix_diff],\n",
    "                            'pos': merged_token['pos']}\n",
    "            merged_token['text'] = merged_token['text'][prefix_diff:]\n",
    "            merged_token['start'] = merged_token['start'] + prefix_diff\n",
    "\n",
    "        suffix_diff = (merged_token['start']+len(merged_token['text'])) - (ann_start+ann_length)\n",
    "        if suffix_diff > 0 :\n",
    "            # cut off suffix and put in new token\n",
    "            suffix_token = {'start': ann_start+ann_length+1,\n",
    "                            'text': merged_token['text'][-suffix_diff:],\n",
    "                            'pos': merged_token['pos']}\n",
    "            merged_token['text'] = merged_token['text'][:-suffix_diff]\n",
    "        new_tokens = [t for t in [prefix_token, merged_token, suffix_token] if t is not None]\n",
    "        \n",
    "        # build modified token list\n",
    "        tokens_with_pos = tokens_with_pos[:token_idxs[0]] + new_tokens + tokens_with_pos[token_idxs[1]+1:]\n",
    "\n",
    "    return tokens_with_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_with_annotations = corpus.get_sentences_with_annotations()\n",
    "assert len(sents_with_annotations) == len(corpus.get_all_sentences())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3191 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/3191 [00:00<09:35,  5.54it/s]\u001b[A\n",
      "  0%|          | 2/3191 [00:00<09:28,  5.61it/s]\u001b[A\n",
      "  0%|          | 3/3191 [00:01<20:18,  2.62it/s]\u001b[A\n",
      "  0%|          | 5/3191 [00:01<16:04,  3.30it/s]\u001b[A\n",
      "  0%|          | 6/3191 [00:01<14:38,  3.63it/s]\u001b[A\n",
      "  0%|          | 7/3191 [00:01<13:23,  3.96it/s]\u001b[A\n",
      "  0%|          | 8/3191 [00:02<26:19,  2.01it/s]\u001b[A\n",
      "  0%|          | 9/3191 [00:03<24:13,  2.19it/s]\u001b[A\n",
      "  0%|          | 10/3191 [00:04<34:42,  1.53it/s]\u001b[A\n",
      "  0%|          | 11/3191 [00:04<26:03,  2.03it/s]\u001b[A\n",
      "  0%|          | 12/3191 [00:04<20:33,  2.58it/s]\u001b[A\n",
      "  0%|          | 13/3191 [00:04<17:28,  3.03it/s]\u001b[A\n",
      "  0%|          | 14/3191 [00:05<15:57,  3.32it/s]\u001b[A\n",
      "  0%|          | 15/3191 [00:05<22:59,  2.30it/s]\u001b[A\n",
      "  1%|          | 16/3191 [00:06<22:55,  2.31it/s]\u001b[A\n",
      "  1%|          | 17/3191 [00:06<22:31,  2.35it/s]\u001b[A\n",
      "  1%|          | 18/3191 [00:06<20:18,  2.60it/s]\u001b[A\n",
      "  1%|          | 19/3191 [00:07<15:59,  3.30it/s]\u001b[A\n",
      "  1%|          | 20/3191 [00:07<14:33,  3.63it/s]\u001b[A\n",
      "  1%|          | 21/3191 [00:07<12:19,  4.29it/s]\u001b[A\n",
      "  1%|          | 22/3191 [00:07<14:16,  3.70it/s]\u001b[A\n",
      "  1%|          | 23/3191 [00:08<18:15,  2.89it/s]\u001b[A\n",
      "\n",
      " 48%|████▊     | 1536/3191 [12:32<10:58,  2.51it/s]  "
     ]
    }
   ],
   "source": [
    "def to_token_dict(token_tuple):\n",
    "    return [{'text': text, 'pos': pos} for text, pos in token_tuple]\n",
    "\n",
    "# tag sentences\n",
    "for item in tqdm(sents_with_annotations):\n",
    "    item = to_token_dict(rrp.tag(item['sentence']))\n",
    "\n",
    "with open('tagged_tokens.txt', 'w') as outf:\n",
    "    outf.wrtie(json.dumps(sents_with_annotations, indent=2))\n",
    "    \n",
    "sents_with_annotations[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([{'pos': 'NNS', 'start': 0, 'text': 'Mechanisms'},\n",
       "   {'pos': 'IN', 'start': 11, 'text': 'of'},\n",
       "   {'pos': 'NN', 'start': 14, 'text': 'acid'},\n",
       "   {'pos': 'NN', 'start': 19, 'text': 'resistance'},\n",
       "   {'pos': 'IN', 'start': 30, 'text': 'in'},\n",
       "   {'pos': 'JJ', 'start': 33, 'text': 'enterohemorrhagic'},\n",
       "   {'pos': 'FW', 'start': 51, 'text': 'Escherichia'},\n",
       "   {'pos': 'FW', 'start': 63, 'text': 'coli'},\n",
       "   {'pos': '.', 'start': 67, 'text': '.'}],\n",
       "  {'annotations': [],\n",
       "   'relations': [],\n",
       "   'sentence': 'Mechanisms of acid resistance in enterohemorrhagic Escherichia coli.'}),\n",
       " ([{'pos': 'JJ', 'start': 0, 'text': 'Enterohemorrhagic'},\n",
       "   {'pos': 'NNS', 'start': 18, 'text': 'strains'},\n",
       "   {'pos': 'IN', 'start': 26, 'text': 'of'},\n",
       "   {'pos': 'FW', 'start': 29, 'text': 'Escherichia'},\n",
       "   {'pos': 'FW', 'start': 41, 'text': 'coli'},\n",
       "   {'pos': 'MD', 'start': 46, 'text': 'must'},\n",
       "   {'pos': 'VB', 'start': 51, 'text': 'pass'},\n",
       "   {'pos': 'IN', 'start': 56, 'text': 'through'},\n",
       "   {'pos': 'DT', 'start': 64, 'text': 'the'},\n",
       "   {'pos': 'JJ', 'start': 68, 'text': 'acidic'},\n",
       "   {'pos': 'JJ', 'start': 75, 'text': 'gastric'},\n",
       "   {'pos': 'NN', 'start': 83, 'text': 'barrier'},\n",
       "   {'pos': 'TO', 'start': 91, 'text': 'to'},\n",
       "   {'pos': 'VB', 'start': 94, 'text': 'cause'},\n",
       "   {'pos': 'JJ', 'start': 100, 'text': 'gastrointestinal'},\n",
       "   {'pos': 'NN', 'start': 117, 'text': 'disease'},\n",
       "   {'pos': '.', 'start': 124, 'text': '.'}],\n",
       "  {'annotations': [],\n",
       "   'relations': [],\n",
       "   'sentence': 'Enterohemorrhagic strains of Escherichia coli must pass through the acidic gastric barrier to cause gastrointestinal disease.'}),\n",
       " ([{'pos': 'VBG', 'start': 0, 'text': 'Taking'},\n",
       "   {'pos': 'IN', 'start': 7, 'text': 'into'},\n",
       "   {'pos': 'NN', 'start': 12, 'text': 'account'},\n",
       "   {'pos': 'DT', 'start': 20, 'text': 'the'},\n",
       "   {'pos': 'JJ', 'start': 24, 'text': 'apparent'},\n",
       "   {'pos': 'JJ', 'start': 33, 'text': 'low'},\n",
       "   {'pos': 'JJ', 'start': 37, 'text': 'infectious'},\n",
       "   {'pos': 'NN', 'start': 48, 'text': 'dose'},\n",
       "   {'pos': 'IN', 'start': 53, 'text': 'of'},\n",
       "   {'pos': 'JJ', 'start': 56, 'text': 'enterohemorrhagic'},\n",
       "   {'pos': 'FW', 'start': 74, 'text': 'E.'},\n",
       "   {'pos': 'FW', 'start': 77, 'text': 'coli'},\n",
       "   {'pos': ',', 'start': 81, 'text': ','},\n",
       "   {'pos': 'CD', 'start': 83, 'text': '11'},\n",
       "   {'pos': 'NN', 'start': 86, 'text': 'O157'},\n",
       "   {'pos': ':', 'start': 90, 'text': ':'},\n",
       "   {'pos': 'NN', 'start': 91, 'text': 'H7'},\n",
       "   {'pos': 'NNS', 'start': 94, 'text': 'strains'},\n",
       "   {'pos': 'CC', 'start': 102, 'text': 'and'},\n",
       "   {'pos': 'CD', 'start': 106, 'text': '4'},\n",
       "   {'pos': 'JJ', 'start': 108, 'text': 'commensal'},\n",
       "   {'pos': 'NNS', 'start': 118, 'text': 'strains'},\n",
       "   {'pos': 'IN', 'start': 126, 'text': 'of'},\n",
       "   {'pos': 'FW', 'start': 129, 'text': 'E.'},\n",
       "   {'pos': 'FW', 'start': 132, 'text': 'coli'},\n",
       "   {'pos': 'VBD', 'start': 137, 'text': 'were'},\n",
       "   {'pos': 'VBN', 'start': 142, 'text': 'tested'},\n",
       "   {'pos': 'IN', 'start': 149, 'text': 'for'},\n",
       "   {'pos': 'PRP$', 'start': 153, 'text': 'their'},\n",
       "   {'pos': 'NNS', 'start': 159, 'text': 'abilities'},\n",
       "   {'pos': 'TO', 'start': 169, 'text': 'to'},\n",
       "   {'pos': 'VB', 'start': 172, 'text': 'survive'},\n",
       "   {'pos': 'JJ', 'start': 180, 'text': 'extreme'},\n",
       "   {'pos': 'NN', 'start': 188, 'text': 'acid'},\n",
       "   {'pos': 'NNS', 'start': 193, 'text': 'exposures'},\n",
       "   {'pos': 'NN', 'start': 204, 'text': 'pH'},\n",
       "   {'pos': 'CD', 'start': 207, 'text': '3'},\n",
       "   {'pos': '.', 'start': 209, 'text': '.'}],\n",
       "  {'annotations': [],\n",
       "   'relations': [],\n",
       "   'sentence': 'Taking into account the apparent low infectious dose of enterohemorrhagic E. coli, 11 O157:H7 strains and 4 commensal strains of E. coli were tested for their abilities to survive extreme acid exposures (pH 3).'})]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tokens, annotations in tqdm(zip(tokens, sents_with_annotations)):\n",
    "    tokens = match_tokens_to_original_sentence(annotations['sentence'], tokens)\n",
    "\n",
    "matched[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
